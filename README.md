<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
this is my learning of PyTorch  

**ok .let's learning pytorn**
##first is the tourial of deeplearning with pytorch for 60 minutes  
###this is some basic operate of tensor in py torch  
###torch 和numpy 之间的转换  
#autograd
##baceward
向后传播是标量向后传播，如果是tensor向后传播，必须要增加一个系数，这个系数的意思就是将这个tensor变成标量，按照一定的权值相加。  
#现在开始学习pytorch的一些例子  
##先用numpy 写了一个两层的神经网络，还是比较简单的。  
##然后用torch的tensor写了一个两层的神经网络  
记得学习率是一个非常重要的影响因素。什么都对了，就是因为学习率出现问题，导致乱码。气死了
##问题  
现在我还是不能够使用atuograd.variable,和autograd.function简单的实现two_layer_net，可能是因为backward 对自己定义的forward 方法，其中线性自己写的话，就会出现不收敛的情况，只能说，backward具有一定的局限性。但是我现在还没有自己定义一个backward，我下次得试试。

#学习路线，就是按照。pytorch-example中的目录走的，还是比较详细。  

在我这里。主要是numpy-torch-atuograd-function-tensorflow-nn-optim-module-dynamic_net  
这条路线是真不错，不仅对比了torch和numpy的区别，还对别了其和tensorflow的区别，用他们分别作了实现。
#另外，在学习中，我发现relu其实是目前最好用的激活函数  
#反向传播时，adam比sgd好用。  
=======
# this is my learning of PyTorch  
ok .let's learning pytorn
## first is the tourial of deeplearning with pytorch for 60 minutes
### this is some basic operate of tensor in pytorch
###  torch 和numpy 之间的转换
#autograd
=======
# this is my learning of PyTorch  
ok .let's learning pytorn
## first is the tourial of deeplearning with pytorch for 60 minutes
### this is some basic operate of tensor in pytorch
###  torch 和numpy 之间的转换
#autograd
>>>>>>> origin/master
>>>>>>> =======
# this is my learning of PyTorch  
ok .let's learning pytorn
## first is the tourial of deeplearning with pytorch for 60 minutes
### this is some basic operate of tensor in pytorch
###  torch 和numpy 之间的转换
#autograd
>>>>>>> origin/master
##  baceward
向后传播是标量向后传播，如果是tensor向后传播，必须要增加一个系数，这个系数的意思就是将这个tensor变成标量，按照一定的权值相加。
#现在开始学习pytorch的一些例子
## 先用numpy 写了一个两层的神经网络，还是比较简单的。
## 然后用torch的tensor写了一个两层的神经网络
记得学习率是一个非常重要的影响因素。什么都对了，就是因为学习率出现问题，导致乱码。气死了
## 问题
现在我还是不能够使用atuograd.variable,和autograd.function简单的实现two_layer_net，可能是因为backward 对自己定义的forward 方法，其中线性自己写的话，就会出现不收敛的情况，只能说，backward具有一定的局限性。但是我现在还没有自己定义一个backward，我下次得试试。
# 学习路线，就是按照。pytorch-example中的目录走的，还是比较详细。
在我这里。主要是**numpy-torch-atuograd-function-tensorflow-nn-optim-module-dynamic_net**
这条路线是真不错，不仅对比了torch和numpy的区别，还对别了其和tensorflow的区别，用他们分别作了实现。
# 另外，在学习中，我发现relu其实是目前最好用的激活函数
# 反向传播时，adam比sgd好用。
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> origin/master
>>>>>>> =======
>>>>>>> origin/master
>>>>>>> =======
>>>>>>> origin/master

# 写了一个卷积循环神经网络。

## 三层的LSTM
* 每一层都是一个时间周期个LSTM Cell
* 每一个Cell 内部都是卷积和直积。
* 输入有一个卷积， hidden 有一个卷积。cell是直积得到的。
* 每一层都可以得到一个输出，输出是一个值。不管是多少个hidden state， 都直接卷积到一维数。
* 最终结果是三层卷积循环神经网络的结果 再把结果做一次卷积的结果。
* 输入是一张三维图，但是我们还要加一维的通道，还要加一维的时间，还要加一维的batch
* **关键**torch 中都是有一个batchsize的，也就是，每一次输入都不是一个单纯的值，它需要多个数据输入，就算是一个数据，也要指明这个表示数据量的维数，不能舍弃。

## 经验总结
* 重要的一点，写程序前，脑袋里面一定要对流程熟悉，不熟悉的话就多画画
* 另外，对于网络中的数据结构一定要清楚，每一步之后的数据结构要清楚，最好在写代码之前在草稿上算好。
* 最后，要学会调试啊，没有用什么很高级的技术，反正我就是哪有问题，一步步缩小问题的范围，数据结构错了就看看size,反正多看看中间结果是没错的。





使用pytorch，写了一个三维的lstm。输入是时间，通道，高度，宽度，长度。详细见神经网络->lstm

# 第一个问题，预测内存爆掉， volatile 发挥奇妙作用

## 问题

当我把数据拷贝到集群上，我做的第一件事就是预测。但是我遇到的第一个问题，就是在预测的时候，内存不停得增长，当我将输入变量的参数，volatile设置为true的时候，内存没有直线增增增了。

## 原因和解决

这个东西有点意思的。volatile和requires_grad可不一样，它是能够传递的，经过它的手的所有option的参数，以及输出都不会保存任何信息，包括输出值的梯度和参数的梯度。而requires_grad 只是说这个变量和option后的输出是否需要梯度，但是option的参数还是会保留梯度。
理论上，每一个batch都需要保存参数和输出的值及他们两的梯度，但是，每一个batch都只保留一次，也就是说。所有的输入输入参数，及参数的梯度，在内存中只有一份，

# 第二个问题，就是train内存爆掉，volatile可就没用了。

## 问题

预测的时候，当然，我们不会做bp，当然不用保存任何的梯度信息，认知做过一遍，全删掉就好了。
但是在训练的时候，兄弟，不能这样，你还要往回走呢，所以volatile就没用了。这条路断了。
但是训练的时候内存也会爆掉，而且这个趋势啊，就是一直涨涨涨，买股票这样就好了。

## 原因

我花了三个小时，把整个pytorch的文档都看了一遍，里面确实有提到过变量替换的问题，但是人家是只在多线程中，没有提到在cuda中，这个东西不好搞。
我想了很久，这个结构里面为什么有值在不停得保存，而且没有销毁内存的操作，可是理论上，我的输入的大小是固定的，参数的大小是固定的，输出大小也是固定的，那么唯一可能的是，某些值保留了多份，那这个可能性就大了。可能是输入，输出，也可能是参数，当然还有可能是参数的梯度
我没有回头去看测试的时候是什么在增加，其实这应该也是解决问题的一条路，毕竟我测试的时候是没有参数更新的。至少是可能排除参数更新的。
其实想想也是，pytorch在设计的时候，怎么可能不考虑内存销毁的问题，不管是参数还是参数的梯度，在更新的时候肯定是会销毁原来的值得，所以我怀疑是输入和输出。

## 解决

所以我做了测试，我测试了pytorch.nn.lstmcell是否也会出现这个问题。为了方便测试， 我没有将数据放在cuda上运行，毕竟太快了。我根本反应不过来，所以我在cpu上运行的。这个就快得多了。当然，我加大了数据量，加大了epoch次数，这样我才看得到效果嘛。另外，也是我后来才意识到的，最重要的，就是一定不能只有一层，要有好几层，所以会初始化三个lstmcell模型，然后将他们在模型外面串起来。
效果是我预想的，确实出现了内存不停涨的情况。那就不是我模型的问题。是这个模型的共性嘛。所以我就开始谷歌了。谷歌很给力啊。很多人遇到了out of memory的情况啊，但是人家是在tensorflow上做的，而且找到了一个解决办法，是比较容易的，就在参数里面改一个值就好了。具体得忘了。
所以我知道，这个东西tensorflow都知道。pytorch怎么可能不知道。我就觉得是不是我把模型一个个分开的原因，这个模型之间的值，我是没有处理的，我不会处理啊， 所以我还是做了一个测试
在torch.nn.lstm，我用了一个例子。这个模型是真的方便，直接输入层数，输入input size, hidden size，它什么都帮你准备好了，一劳永逸啊。你猜结果怎么样， 当然是没有增加啊。我还是在cpu上跑的，内存在一开始的时候爆张，但是下一秒，就稳定了。对嘛，这才是理想的情况嘛，所以我就知道了。人家模型里面是会做这一步的，就是清除多余的内存。那就简单了嘛。我直接再写一个模型，把三个模型合起来就好了。对的。对的。结果也是我想的，内存保持不变了。

# 第三个问题。这还有的显存是哪里的，

# 问题

先说明，我的意思不是说你要没内存才好，而是，这些内存是怎么产生的，因为我没告诉你我的数据有17g。我这么大的数据，我只有11g的gpu，怎么搞，所以，灵感，真的，我就觉得内存只和batch_size和参数大小以及参数的梯度大小有关。

# 原因

所以我做了一个测试，就是慢慢增加数据的大小，比如原来是500，现在是1000。但是一点，batch_size是不变，我都用的50, 结果也是正如我想的那样，显存是一点点都没增加（这里强调一点，文件读取的时候，是一次性全部读取到内存中，但是在close后那个文件流是关闭的，但是我是保存了数据的，所以现在在内存的是我保存的数据结构。但是这只是在内存中，我还没有全部提交到显卡上。但是我提交到显卡并不是一次性全部提交，我是一个batch一个batch的提交，而且，我使用的是用一个变量提交，我不知道是不是同一个变量有没有区别，理论上应该是没有区别的。所以在显卡中只有一个batch的数据大小）
所以问题都解决了。产生的内存是来自batch和参数的。那就好办多了。我的文件虽然有17g，但是和显存无关啊。我设置个合适的大小，100,差不多十个g。嗯。够了。

# 第四个问题，是我自己的问题。感觉自己的能力太弱了，虽然好像可以纠正。

哎，改一行代码，就忘了改掉相关的代码，其实我在走神，你信吗，我也就现在没走神。我在码代码的时候，脑子不停得想着侯亮平，高育良。哎哟，不是我想，是剧情在我脑袋里转。我其实也不想想，但是我不知道想什么，我感觉是我写代码的时候没组织好结构，想想也不知道怎么想，有点乱。这跟我的逻辑思维能力弱有关吧。我结果问题，好像没有逻辑，全凭感觉。所以， 以后的训练中，很重要的一点，就是组织好逻辑结构，脑袋里面构思清楚这个层次结构，这是最简单的结构嘛。

# 问题五，训练的学习率，epoch大小，batch_size 超参的确定，优化算法的确定

这些超参都是网络外的东西，但是我发现也非常重要

## 问题

当我进行训练的时候，首先设置学习率为0.001,优化函数使用adam，一个batch是100,总误差是下降得非常慢的，而且起始误差就有两百的，天啊，优化到什么时候去了。我心灰意冷的时候，就c-c，直接中断了。

- 学习率到底是怎么决定的，有公式吗，还是要一个个试试。
- batch_size有多大最好，这种局部下的全局方式有多少优势
- 当我使用一个batch的时候，每个batch都在优化，但是优化的方向是不一样的，因为每个batch都有自己的局部最优，但是所有的数据都有一个共同的全局最优，而且理论也证明是这样的，但是我的问题是，我一直使用一样的顺序训练，会有印象。

## 原因

我想了两个法子：

- 一个就是先用一千个数据训练，优化出一个比较好的初始化参数，然后再在这个参数的基础上使用全部的数据训练。想想嘛，在这一千个数据你都不错，在一玩个上肯定也差不了多少，
- 另外一个方法就是，我提高学习率，你学得慢，我让你学得快一点，但是，我真的不知道你会不会太大了，我只有试试。
- 还有一点，我这个batch到底要怎么选，选太大，你不可能啊，存不下，选太小，你容易陷入到局部最优啊，难，

最终，我啊，没改batch_size，我将两个方法合起来了。先训练了一个一千的数据，得到一个比较小的初始值，然后，提高学习率到0.01，然后再在所有的数据上跑。结果，我看到的是初始化误差没有变化，我还看到的是，学习速率非常慢。
这就说明一个很严重的问题，小数据有小数据的特征，大数据有大数据的特征，我怎么可能让他们相提并论，这个观点是错的。
另外，优化算法式作用在一个batch上的，所以，对每个batch她都一样的概念，使用batch gradient decent是一个局部和全局的综合方法，但是怎么说也是有局部的，所以优化速度很慢，最快的还是sgd，毕竟人家是沿着全局梯度走的，你局部都不知道跑哪去了。

# 问题六,在解决问题五之前，我先解决程序并行化的问题。

## 问题

至少需要在读取文件的时候并行化，不然就太慢了。17g的文件，要读二十多分钟，事实上，两分钟应该就能解决。为了方便并行，减少通信，我直接使用split -l 500 train.txt -d -a 2 train_将train这个17g的数据集按照每五百行的方式，分成20个文件，每个文件用train_开头命名，以两位的数字结尾。还有一个就是gpu并行

## 原因和解决

- gpu并行还是很好做的，

```
model = torch.nn.DataParalle(model, devices=[0,1])

```

- 使用多线程处理，在python中时不能实现的，python 自带了一个全局解释锁，所有的都不能同时执行，所以当用thread活着threading的时候，是不能达到提高读取效率的效果的。
- 然后我就不知道怎么办了，不过我想啊，总不能在python 中不能并行吧。这不科学。所以啊。我就去找了，原来多线程不可以，多进程时可以的，这就好啊，而且多进程之间我是不用通信的，我都读的是不同的文件，当然不用通信。但是有一个问题，我读的是不同的文件，但是，我写入的是同一个数据结构啊。这可咋办？
- 每个进程都是一个单独的变量也就是说，我每个进程中的变量是不通的，看起来好像是同一个变量，但是不能存在一起啊， 他们是不同的，我得把他们合起来才行。我想啊，我每个函数都是readfile，我把函数的返回值获取不就好了，但是事实上这一切都只是我想想。当我需要使用一个进程中的变量的时候，我要怎么把数据传过去
  。queue是一个不错的方法，照正常的道理说，这样是可以的，但是我总是读不到值，值都没了。

~~~Python
from multiprocessing import Queue, Process

def f(q):

  print('this is process')

  for i in range(20):

    q.put(i)

  q.put(None)#这个也是后面的修改，有了这个后面才能判断是不是全部写完了，这也是一定要一个进程一个queue的原因，非常方便控制进程

  return 1

#q = Queue()之前是这样写的

q = []#这是后面做的修改

thread = []

tt = []

for i in range(10):

  q.append(Queue())#这是后面的修改

for i in range(10):

  thread.append(Process(f, args=(q,)))

for i in range(10):

  thread[i].start()

for i in range(10):

  while True:

    t1 = q.get()

    if t1 is None:#当某一个进程结束的时候，就给一个none,再一个break，这次回话就结束了

      break

    tt.append(t1)

for i in range(10):

  thread[i].join()


~~~

- 所以我想了另外一个方法，Pool，它有一个map函数，能够自动根据pool进行多进程编程。但是它并不是说能做到两个进程运行一件事，必须一个任务一个进程才有效果。

~~~python
from multprocessing import Pool

p = Pool(4)

s = p.map(f,[q1, q2, q3, q4])#使用一个map函数，能够做到一一对应，但是对我来说更重要的是，s是函数f的返回值，而且也是一个list,对应了每个q。

~~~





- 后来我对这个queue做了个修改，多个进程同时对一个q写入，我怕会冲突，所以每个线程都给了一个队列来做消息传递。同时，queue也是有一个int的长度，是有大小的，但是queue不一样的是，它在不停地往里面写，也在不停地取出来，所以只要减小每次放入queue的量的大小，就能保证将所有的数据读完，而且不会出现超出queue范围的情况。在对queue做for循环的时候，我还想这时候是一个个queue来读取的，会不会慢，后来想想这都在高速缓存去了，这怎么慢，想慢都不行。
- 这个就是最后的结果了。原来读取17g要1300s。现在只用250s。这个加速比有5.2。不可思议，而且这中间还有我对数据的处理。简直爆炸。这个问题已经过去四天了，终于做好了。
- 多进程有一点很重要，当一个进程开始的时候，主进程是会继续运行的，而子进程也会运行，这时候是一个并行的过程，所以在子进程和父进程的通信中，当queue中没有值得时候，父进程需要等待，所以这里必须使用一个while true，而终结的判断方法就是子进程放入了一个none。

# 总结

- 这次试验，不仅对网络结构内部运行了解更透彻了， 对pytorch的运行方式也了解得更多了，还有数据处理的方式，代码组织方式，调试方法，都有了很多的了解。这样挺好，真的，我能切实感受到自己的成长，这个东西对我有用，是大大的鼓舞，是我继续学下去还能有动力的原因。
- 另外，不能打单纯的想理论了，只想理论根本就想不到还有这么多的问题，做实验一般的时间都花在了无用的地方。但也是一种能力嘛。
- 其实创新和灵感我感觉不是无缘无故的临时想法，也是有一定的逻辑，因为你心中想解决某个问题，所以才跳出来的灵感，所以灵感其实很需要积累的。
- ​

# 小技巧

- Scp 从一个用户传到另外一个用户

~~~shell
scp train.py yangchao@node6:~/
~~~

将文件发送到杨超账户下的node6节点下的家目录

- vsp train.py 在vim中垂直分栏显示另一个文件


- ctrl-z将当前进程放到后台运行。fg就能返回刚才的进程

tar -zxvf 解压

tar -zcvf 压缩

# 最后，再来解决node5的问题

- 我先看了epoch次数，发现次数越小效果越好，这个问题就大了，我光初始化结果就很好了。明显这是不科学的。
- 我就改了学习率，学习率过大也是会发生这个现象的，跑过了最低值，向反方向跑了，



# 最后的总结

结果真的不好，我使用了三个模型，有lstm，有cnn，有ann。不管什么方式，最大的问题就是过拟合。也就是从图像中不能简单地学习到降雨量知识，有很多地方都具有冗余性。所以最大的问题就是提取特征。对于特征的提取，只要得到我们想要的特征，就能对特征分类，或者回归，但是这个特征一定要是我们需要的特征，一定要是跟结果相关的特征，所以提取特征就是关键的一步。

没错，我们说神经网络是一个黑盒子，但是它真的不是给一个输入，给一个标签，就能够训练得到好的假设，这个假设可能具有一定的参考性，但是能够得到的效果是有限的，但是如果能得到好的特征，代表数据的和结果有关的特征，那么肯定是能得到一个比较好的结果。

所以：

* 神经网络能够使黑盒子，是因为它能够提取特征。但是这个特征是有限的，它不一定是我们需要的特征，或者说它提取到的特征是不明显的。我们把神经网络看成一个黑盒子，就是希望它能够自动提取到特征，提取到相关的特征，提取到我们也不知道怎么就提取到的特征，但是真的很有效果的一个算法实现。
* 神经网络能拟合函数，是基于有好的特征的。当我们给一个神经网络的输入不是原始数据而是处理后的数据特征，那么我们总是希望能够学会一个假设，这个假设能够从特征映射到预期结果。所以神经网络能表现很好的地方，是它能够很完美地拟合函数，但是一定要是很好的额特征作为输入。
* 当然，我们很多时候是把上面两种结果放在一个神经网络中实现的。我们希望神经网络又能够提取特征，又能够拟合函数。反过来思考，能得到预期结果，那么它肯定是拟合了一种函数，那么它肯定是提取了好的特征，不然不相关的特征也得不到结果。事实上神经网络也是这样做的。
* 但是过程的复杂性就在于，它真的能够提取到我们想要的特征吗，或者说，这个特征或不会很“模糊”。
* 这样一来就牵扯出一些列的问题：
  * 神经网络的层数是不是可以理解过前面几层在做特征提取，后面几层在做函数拟合。还是混合在一起了。
  * 如果是前者，那么到第需要多少层来提取特征，多少层来拟合函数。



神经网络希望能够模拟人类的大脑神经工作方式，从一个黑盒子中得到结果。但是人类的视觉系统（比如说），是有一系列的物理基础的，它能够提取到好的特征，都是根据物理规律的，而神经元，也许就是一个传递的功能，真正发挥做种的应该是功能区。

所以，神经网络是一个算法，这个算法就类似于机器学习，再直白一点就是数学建模，一个好的模型，一定是来源于一个好的物理现实，依据理论而科学的物理规律。